{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f871ff4b",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "1. 回归算法(Regression)\n",
    "2. 分类算法(Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676a61c",
   "metadata": {},
   "source": [
    "监督学习的关键特征：\n",
    "1. 为模型提供一个包含特征和目标变量的训练数据集。让模型去学习如何映射特征到目标变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c05f6f",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "1. 聚类算法 (Clustering Algorithms)\n",
    "2. 异常检测算法 (Anomaly Detection Algorithms)\n",
    "3. 降维算法 (Dimensionality Reduction Algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bd2a7",
   "metadata": {},
   "source": [
    "#### 成本函数（cost function）\n",
    "#### 成本函数的本质就是，衡量模型预测值与真实值之间的差距。\n",
    "#### 线性回归的目标就是找到合适的$w$值，使得代价函数$J(w)$最小。\n",
    "1. 在训练模型时，我们希望模型能够拟合训练数据，使得模型的预测值与真实值之间的差距最小，也就是希望模型的预测值尽可能接近真实值。\n",
    "2. 为了衡量模型的拟合程度，我们需要定义一个成本函数（cost function），用来衡量模型预测值与真实值之间的差距。\n",
    "3. 模型的函数形式为$h(x,w)=wx+b$，其中$x$为输入向量，$w,b$为模型参数，$y$为真实值。预测值$h(x,w)$与真实值$y$之间的差距可以表示为：$J(w) = h(x,w) - y$。\n",
    "4. 我们希望找到一个最优的$w$值，使得$J(w)$最小。\n",
    "4. 可以理解为，成本函数是关于$w,b$的函数，$w,b$的变化会影响$J(w)$的大小。\n",
    "6. 优化参数的过程，就是找到使得$J(w)$最小的$w$值。\n",
    "3. 常见的成本函数有：\n",
    "    - 平均平方误差（mean squared error）：$J(w) = \\frac{1}{2m}\\sum_{i=1}^m(h(x_i,w)-y_i)^2$\n",
    "    - 绝对值误差（absolute error）：$J(w) = \\sum_{i=1}^m|h(x_i,w)-y_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd66bc",
   "metadata": {},
   "source": [
    "![image.png](./imges/模型_代价函数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bfbc0",
   "metadata": {},
   "source": [
    "#### 梯度下降（Gradient Descent）\n",
    "\n",
    "✅ 一句话：沿着“代价函数”最陡的下坡方向，一小步一小步走，直到走到（或接近）最低点。\n",
    "\n",
    "---\n",
    "\n",
    "📌 核心要点（先看这里）\n",
    "- 目标：最小化代价函数 J（预测与真实的差距）。\n",
    "- 方向：顺着“梯度的反方向”。梯度＝最陡上坡方向，反过来就是下坡。\n",
    "- 方式：迭代更新参数，每次让 J 再小一点。\n",
    "\n",
    "---\n",
    "\n",
    "🧭 怎么做（动手 5 步）\n",
    "1. 初始化参数（例如 w、b）。\n",
    "2. 计算当前代价 J。\n",
    "3. 计算梯度（每个参数的斜率）：∂J/∂w、∂J/∂b。\n",
    "4. 按学习率 α 更新参数（往反方向走）：\n",
    "   - w ← w − α·(∂J/∂w)\n",
    "   - b ← b − α·(∂J/∂b)\n",
    "5. 重复 2–4，直到 J 几乎不再下降或达到迭代上限。\n",
    "\n",
    "---\n",
    "\n",
    "🎚 学习率 α（成败关键）\n",
    "- 过大：来回乱跳，甚至发散。\n",
    "- 过小：走得很慢，训练时间很长。\n",
    "- 实操建议：从 0.1 / 0.01 / 0.001 试起，观察 J 是否“平稳且持续下降”。\n",
    "\n",
    "---\n",
    "\n",
    "🧩 常见坑与小技巧\n",
    "- 先做特征缩放/标准化（如 StandardScaler），各维度同量纲更易收敛。\n",
    "- 画 J 的下降曲线：若锯齿或上升，多半 α 太大。\n",
    "- 三种变体：\n",
    "  - 批量（Batch）：每次用全量数据，稳定但慢。\n",
    "  - 小批量（Mini-batch）：速度与稳定性平衡（最常用）。\n",
    "  - 随机（SGD）：每次 1 条样本，抖动大但更容易跳出局部最优。\n",
    "\n",
    "---\n",
    "\n",
    "🧪 超简例：线性回归 y ≈ w·x + b（用均方误差 MSE 做 J）\n",
    "- 误差：eᵢ = (w·xᵢ + b − yᵢ)\n",
    "- 梯度近似：\n",
    "  - ∂J/∂w ∝ 平均(eᵢ·xᵢ)\n",
    "  - ∂J/∂b ∝ 平均(eᵢ)\n",
    "- 参数更新：\n",
    "  - w ← w − α·平均(eᵢ·xᵢ)\n",
    "  - b ← b − α·平均(eᵢ)\n",
    "\n",
    "---\n",
    "\n",
    "🛑 什么时候停\n",
    "- 连续多次迭代，J 的下降幅度 < 很小阈值（如 1e−6）。\n",
    "- 梯度范数很小（接近“平地”）。\n",
    "- 达到最大迭代次数。\n",
    "\n",
    "—— 总结：梯度下降就是“顺着山坡往低处走”，每一步都让代价更小，直到几乎走不动为止。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
